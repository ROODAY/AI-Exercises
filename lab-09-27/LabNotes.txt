Highlights of Lab0927
1. Discuss the general workflow of a classification task.
	a) Talk about the X -> [black box] -> Y structure: the shape of X; what's in the box (fit(XTrain, YTrain) and predict(XTest)).
	b) Talk about preparation: how K(5) fold cross-validation works; how to maintain proper sample amounts in each fold (separate samples by class to subsets, shuffle all if them subsets, then pick 20% from each class).
	c) Evaluation. Students needs to write the evaluation functions (confusion matrix and various metrics) by themselves.
	
2. Discuss the AoS network. Walk through the AoS network. Need to redo an example on the blackboard. Point out the following.
	a) "fire" = output 1 or True; "not fire" = output 0 or False.
	b) This network does not learn or recognize any pattern. Because the relation among the inputs is known, it simply translate the relation into a network form.
	
3. Implement the AoS network from scratch to V1. And then modify to change it to V2 (this will be the skeleton code for P1).

	I. scratch to V1:
	a) Encode individual, labels, and a sample input.
	b) getWeightedSum(X, W) and threshold(S, T).
	c) outputP = [output1, output2]
	d) Do the similar for the second layer, now change the input to outputP.
	e) outputM has two components, because the answers are mutually exclusive, only one of them is 1 and the other must be 0, but in general people use argmax to find the label because usually a NN outputs probabilities.
	f) argmax
	
	II. V1 to V1a:
	Talk about matrix multiplication first, just in case. This can be accomplished using 3 nested loops.
	a) Point out that missing links are links with weights = 0, then change x[:3] to x and some related parts.
	b) We want to do this in one line: output1 = Introduce numpy vectors to x and W. Then we have output1 = threshold(x * W1[:, 0], 0.5).
	c) S1 = x * W1 = [x * W1[:, 0], x * [:, 1]], we want to to something like T1 = [0.5, 0.5] and outputP = threshold(S1, T1), so we need to change threshold fuction to handle a list of inputs (check S1.shape first).
	d) Now outputP = threshold(S1, T1). Print it out and see what each row means in the network graph.

	III. V1a to V1b:
	a) Do the similar thing for the magenta node layer (up to outputM = threshold(S2, T2)).
	b) Run it. Whoops! argmax complains!
	c) print(outputM) shows that outputM is a matrix while L in argmax(L) must be a list! But let's just leave it as a matrix and use outputM.argmax(axis = 1) to get argmax per row.
	d) Expand x to X, and replace all x to X. What does outputM look like now? Each row in outputM correspond to each sample. Hence .argmax(axis = 1) function is handy.
	e) Change the final line to show all results.
	
	IV. V1b to V2:
	a) Now group W1 and W2 and Ws, and group T1 and T2 to Ts. Then change W1 to Ws[0] etc.. Merge S1 = x * W1 and outputP = (S1, T1).
	b) Build the NeuralNetwork class.
	c) Point out the general structure of the class (what functions must be included).
	d) We only need to code __init__, predict and forward here.
	e) Move outputP = and outputM = to forward. Because they are literally doing forward propagation.
	f) Add a few lines (.argmax(axis = 1) part) to predict.
	f) In __init__, the "fire" function is usually called activation function, which is differentiable. Here we are using thresholding, which is not differentiable, but let's just put deltaActivate here for now.
	g) Now do NN = NeuralNetwork() etc.. Done! (almost)
	
	V. V2 to P1Skeleton
	a) Point out that Ws should be set up in fit, not __init__ and NNodes should be passed to __init__. This is because usually the model sees the input and output size only in the training phase.
	b) Bias node is represented as +1 in Ws[1].